{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyenchant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OJhC18FGj6r",
        "outputId": "920f71c9-d902-4ff4-a5e3-5f63d4269290"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSEG9bIbGxd9",
        "outputId": "79a6b6c0-63d9-42bc-acc0-8175992b565c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.2 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "import string\n",
        "import enchant\n",
        "import textstat\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK8a-5PLGKoI",
        "outputId": "23062921-3efb-407e-84e5-5e0e8006e202"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing and Processing Input Excel File\n",
        "URL_List = pd.read_excel(\"Input.xlsx\")\n",
        "urls = URL_List[\"URL\"].tolist()"
      ],
      "metadata": {
        "id": "isMAT2iVGM1e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Text files for Positive, Negative and Stop Words\n",
        "with open(\"positive-words.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    pw = file.read()\n",
        "    positive_words = pw.split()\n",
        "\n",
        "with open(\"negative-words.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    nw = file.read()\n",
        "    negative_words = nw.split()\n",
        "    \n",
        "with open(\"StopWords_Auditor.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    sa = file.read()\n",
        "    stopwords_auditor = sa.split()\n",
        "    \n",
        "with open(\"StopWords_Currencies.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    sc = file.read()\n",
        "    stopwords_currencies = sc.split()\n",
        "\n",
        "with open(\"StopWords_DatesandNumbers.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    sdn = file.read()\n",
        "    stopwords_datesandnumbers = sdn.split()\n",
        "    \n",
        "with open(\"StopWords_Generic.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    sg = file.read()\n",
        "    stopwords_generic = sg.split()\n",
        "    \n",
        "with open(\"StopWords_GenericLong.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    sgl = file.read()\n",
        "    stopwords_genericlong = sgl.split()\n",
        "    \n",
        "with open(\"StopWords_Geographic.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    sge = file.read()\n",
        "    stopwords_geographic = sge.split()\n",
        "    \n",
        "with open(\"StopWords_Names.txt\",\"r\",encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    sn = file.read()\n",
        "    stopwords_name = sn.split()\n",
        "\n",
        "## Combining all the stopwords\n",
        "stopwords = stopwords_auditor + stopwords_currencies + stopwords_datesandnumbers + stopwords_generic + stopwords_genericlong + stopwords_geographic + stopwords_name\n",
        "\n",
        "# Creating an empty list to store the output\n",
        "output = []"
      ],
      "metadata": {
        "id": "s5-2IA7kGOQZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Extraction loop\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    article_text = soup.get_text()\n",
        "    \n",
        "    # Calculating Variables\n",
        "    ## Positive Score\n",
        "    ps = 0\n",
        "    for word in article_text.split():\n",
        "        if word in positive_words:\n",
        "            ps += 1\n",
        "    \n",
        "    ## Negative Score\n",
        "    ns = 0\n",
        "    for word in article_text.split():\n",
        "        if word in negative_words:\n",
        "            ns += 1\n",
        "            \n",
        "    ## Removing Stopwords\n",
        "    def clean_text(text):\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        clean_tokens = [word.lower() for word in tokens if word.lower() not in stopwords and word.lower() not in string.punctuation]\n",
        "        return \" \".join(clean_tokens)\n",
        "    cleaned_text = clean_text(article_text)\n",
        "    \n",
        "    ## Average Sentence Length\n",
        "    sentences = nltk.sent_tokenize(cleaned_text)\n",
        "    words = nltk.word_tokenize(cleaned_text)\n",
        "    total_words = len(words)\n",
        "    total_sentences = len(sentences)\n",
        "    asl = total_words/total_sentences if total_words > 0 else 0\n",
        "\n",
        "    def is_complex_word(word):\n",
        "        return nltk.pos_tag([word])[0][1] in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    \n",
        "    def complex_words(text):\n",
        "        words = nltk.word_tokenize(text)\n",
        "        return [word for word in words if is_complex_word(word)]\n",
        "    \n",
        "    def get_percentage_complex_words(text):\n",
        "        d_cmu = cmudict.dict()\n",
        "        d_enchant = enchant.Dict(\"en_US\")\n",
        "        words = text.split()\n",
        "        def syllable(word):\n",
        "            try:\n",
        "                return [len(list(y for y in x if y[-1].isdigit())) for x in d_cmu[word]][0]\n",
        "            except KeyError:\n",
        "                return 0\n",
        "        complex_words = [word for word in words if syllable(word) > 2 and d_enchant.check(word)]\n",
        "        total_words = len(words)\n",
        "        return 0 if total_words == 0 else len(complex_words) / total_words * 100\n",
        "    \n",
        "    pcw = get_percentage_complex_words(cleaned_text)\n",
        "    \n",
        "    ## FOG Index\n",
        "    def flesch_kincaid_index(text):\n",
        "        words = nltk.word_tokenize(text)\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        num_words = len(words)\n",
        "        num_sentences = len(sentences)\n",
        "        syllables = 0\n",
        "        for word in words:\n",
        "            syllables += textstat.syllable_count(word)\n",
        "        avg_sent_len = num_words / num_sentences\n",
        "        avg_syllables_per_word = syllables / num_words\n",
        "        return 0.39 * avg_sent_len + 11.8 * avg_syllables_per_word - 15.59\n",
        "    fogi = flesch_kincaid_index(cleaned_text)\n",
        "    \n",
        "    ## Average of Words per sentence\n",
        "    words = nltk.word_tokenize(cleaned_text)\n",
        "    total_words = len(words)\n",
        "    sentences = nltk.sent_tokenize(cleaned_text)\n",
        "    total_sentences = len(sentences)\n",
        "    asl = total_words/total_sentences if total_words > 0 else 0\n",
        "    \n",
        "    ## Complex Word Count  \n",
        "    def get_complex_word_count(text):\n",
        "        d = cmudict.dict()\n",
        "        d_enchant = enchant.Dict(\"en_US\")\n",
        "        def syllable(word):\n",
        "            try:\n",
        "                return [len(list(y for y in x if y[-1].isdigit())) for x in d[word]][0]\n",
        "            except KeyError:\n",
        "                return 0\n",
        "        words = text.split()\n",
        "        complex_words = [word for word in words if syllable(word) > 2 and d_enchant.check(word)]\n",
        "        return len(complex_words)\n",
        "    comp = get_complex_word_count(cleaned_text)\n",
        "    \n",
        "    # Syllable per word calculation\n",
        "    def count_syllables(word):\n",
        "        vowels = \"aeiouyAEIOUY\"\n",
        "        count = 0\n",
        "        word = word.lower()\n",
        "        if word[0] in vowels:\n",
        "            count +=1\n",
        "        for index in range(1,len(word)):\n",
        "            if word[index] in vowels and word[index-1] not in vowels:\n",
        "                count +=1\n",
        "        if word.endswith(\"e\"):\n",
        "            count -= 1\n",
        "        if word.endswith(\"le\"):\n",
        "            count+=1\n",
        "        if count == 0:\n",
        "            count +=1\n",
        "        return count\n",
        "    syllable = count_syllables(cleaned_text)\n",
        "\n",
        "    # Average word length calculation\n",
        "    def average_word_length(text):\n",
        "        words = nltk.word_tokenize(text)\n",
        "        total_letters = sum(len(word) for word in words)\n",
        "        return total_letters / len(words) if len(words) > 0 else 0\n",
        "\n",
        "    # Pronoun counting \n",
        "    def count_personal_pronouns(text):\n",
        "      personal_pronouns = \"I|me|you|he|him|she|her|it|we|us|they|them\"\n",
        "      count = len(re.findall(f'\\\\b({personal_pronouns})\\\\b', text))\n",
        "      return count\n",
        "\n",
        "    pronoun_count = count_personal_pronouns(cleaned_text)\n",
        "\n",
        "    # Calculate average word length\n",
        "    awl = average_word_length(cleaned_text)\n",
        "        \n",
        "    # Performing Text Analysis\n",
        "    blob = TextBlob(article_text)\n",
        "    \n",
        "    # Compute the Variables\n",
        "    positive_score = ps\n",
        "    negative_score = ns\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "    avg_sentence_length = asl\n",
        "    percentage_of_complex_words = pcw\n",
        "    fog_index = fogi\n",
        "    avg_number_of_words_per_sentence = asl\n",
        "    complex_word_count = comp\n",
        "    word_count = len(blob.words)\n",
        "    syllable_per_word = syllable\n",
        "    personal_pronouns = pronoun_count\n",
        "    avg_word_length = awl\n",
        "\n",
        "    # Saving the output for each URL\n",
        "    output.append([url, positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
        "              percentage_of_complex_words, fog_index, avg_number_of_words_per_sentence, complex_word_count, word_count,\n",
        "              syllable_per_word, personal_pronouns, avg_word_length])"
      ],
      "metadata": {
        "id": "7dJtvsVjGRyT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting the Output as Excel file\n",
        "df = pd.DataFrame(output, columns=[\"URL\", \"positive_score\", \"negative_score\", \"polarity_score\", \"subjectivity_score\", \n",
        "                                   \"avg_sentence_length\", \"percentage_of_complex_words\", \"fog_index\", \n",
        "                                   \"avg_number_of_words_per_sentence\", \"complex_word_count\", \"word_count\", \n",
        "                                   \"syllable_per_word\", \"personal_pronouns\", \"avg_word_length\"])\n",
        "df.to_excel(\"output.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "libG1mJFGVMP"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}